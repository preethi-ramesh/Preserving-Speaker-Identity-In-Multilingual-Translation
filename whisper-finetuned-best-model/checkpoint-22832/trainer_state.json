{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 22832,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0700770847932726,
      "grad_norm": 264.8693542480469,
      "learning_rate": 8.923134197617379e-09,
      "loss": 2.6284,
      "step": 200
    },
    {
      "epoch": 0.1401541695865452,
      "grad_norm": 277.5022277832031,
      "learning_rate": 8.844297477224947e-09,
      "loss": 2.5683,
      "step": 400
    },
    {
      "epoch": 0.2102312543798178,
      "grad_norm": 252.46693420410156,
      "learning_rate": 8.765460756832515e-09,
      "loss": 2.5357,
      "step": 600
    },
    {
      "epoch": 0.2803083391730904,
      "grad_norm": 203.41180419921875,
      "learning_rate": 8.686624036440085e-09,
      "loss": 2.4697,
      "step": 800
    },
    {
      "epoch": 0.350385423966363,
      "grad_norm": 149.03846740722656,
      "learning_rate": 8.608181499649615e-09,
      "loss": 2.4038,
      "step": 1000
    },
    {
      "epoch": 0.4204625087596356,
      "grad_norm": 166.1772003173828,
      "learning_rate": 8.529344779257183e-09,
      "loss": 2.3743,
      "step": 1200
    },
    {
      "epoch": 0.4905395935529082,
      "grad_norm": 193.91989135742188,
      "learning_rate": 8.450508058864751e-09,
      "loss": 2.3333,
      "step": 1400
    },
    {
      "epoch": 0.5606166783461808,
      "grad_norm": 221.02835083007812,
      "learning_rate": 8.371671338472319e-09,
      "loss": 2.2996,
      "step": 1600
    },
    {
      "epoch": 0.6306937631394534,
      "grad_norm": 120.60548400878906,
      "learning_rate": 8.292834618079887e-09,
      "loss": 2.264,
      "step": 1800
    },
    {
      "epoch": 0.700770847932726,
      "grad_norm": 122.80549621582031,
      "learning_rate": 8.213997897687455e-09,
      "loss": 2.2766,
      "step": 2000
    },
    {
      "epoch": 0.7708479327259986,
      "grad_norm": 107.83150482177734,
      "learning_rate": 8.135161177295024e-09,
      "loss": 2.2041,
      "step": 2200
    },
    {
      "epoch": 0.8409250175192712,
      "grad_norm": 146.72447204589844,
      "learning_rate": 8.056324456902592e-09,
      "loss": 2.1656,
      "step": 2400
    },
    {
      "epoch": 0.9110021023125437,
      "grad_norm": 113.12567138671875,
      "learning_rate": 7.977487736510162e-09,
      "loss": 2.1516,
      "step": 2600
    },
    {
      "epoch": 0.9810791871058164,
      "grad_norm": 115.53015899658203,
      "learning_rate": 7.898651016117728e-09,
      "loss": 2.1099,
      "step": 2800
    },
    {
      "epoch": 1.051156271899089,
      "grad_norm": 98.9894027709961,
      "learning_rate": 7.819814295725298e-09,
      "loss": 2.0778,
      "step": 3000
    },
    {
      "epoch": 1.1212333566923616,
      "grad_norm": 92.49610137939453,
      "learning_rate": 7.740977575332866e-09,
      "loss": 2.0794,
      "step": 3200
    },
    {
      "epoch": 1.1913104414856341,
      "grad_norm": 106.11005401611328,
      "learning_rate": 7.662140854940434e-09,
      "loss": 2.034,
      "step": 3400
    },
    {
      "epoch": 1.2613875262789067,
      "grad_norm": 100.32170104980469,
      "learning_rate": 7.583304134548002e-09,
      "loss": 2.0133,
      "step": 3600
    },
    {
      "epoch": 1.3314646110721795,
      "grad_norm": 86.63706970214844,
      "learning_rate": 7.50446741415557e-09,
      "loss": 2.0163,
      "step": 3800
    },
    {
      "epoch": 1.401541695865452,
      "grad_norm": 89.76776123046875,
      "learning_rate": 7.426024877365102e-09,
      "loss": 1.9931,
      "step": 4000
    },
    {
      "epoch": 1.4716187806587246,
      "grad_norm": 107.869384765625,
      "learning_rate": 7.347188156972669e-09,
      "loss": 2.0053,
      "step": 4200
    },
    {
      "epoch": 1.5416958654519972,
      "grad_norm": 107.68012237548828,
      "learning_rate": 7.268351436580238e-09,
      "loss": 1.9739,
      "step": 4400
    },
    {
      "epoch": 1.6117729502452698,
      "grad_norm": 86.48738861083984,
      "learning_rate": 7.1895147161878065e-09,
      "loss": 1.9633,
      "step": 4600
    },
    {
      "epoch": 1.6818500350385424,
      "grad_norm": 112.13788604736328,
      "learning_rate": 7.110677995795374e-09,
      "loss": 1.9646,
      "step": 4800
    },
    {
      "epoch": 1.751927119831815,
      "grad_norm": 92.13935089111328,
      "learning_rate": 7.031841275402943e-09,
      "loss": 1.905,
      "step": 5000
    },
    {
      "epoch": 1.8220042046250877,
      "grad_norm": 89.9231185913086,
      "learning_rate": 6.953004555010511e-09,
      "loss": 1.9208,
      "step": 5200
    },
    {
      "epoch": 1.89208128941836,
      "grad_norm": 85.4288558959961,
      "learning_rate": 6.87416783461808e-09,
      "loss": 1.8795,
      "step": 5400
    },
    {
      "epoch": 1.9621583742116329,
      "grad_norm": 80.12664031982422,
      "learning_rate": 6.795331114225647e-09,
      "loss": 1.8844,
      "step": 5600
    },
    {
      "epoch": 2.032235459004905,
      "grad_norm": 84.27781677246094,
      "learning_rate": 6.716494393833216e-09,
      "loss": 1.8718,
      "step": 5800
    },
    {
      "epoch": 2.102312543798178,
      "grad_norm": 89.53733825683594,
      "learning_rate": 6.6376576734407845e-09,
      "loss": 1.8563,
      "step": 6000
    },
    {
      "epoch": 2.172389628591451,
      "grad_norm": 86.16024017333984,
      "learning_rate": 6.558820953048353e-09,
      "loss": 1.8253,
      "step": 6200
    },
    {
      "epoch": 2.242466713384723,
      "grad_norm": 83.7630844116211,
      "learning_rate": 6.479984232655921e-09,
      "loss": 1.8283,
      "step": 6400
    },
    {
      "epoch": 2.312543798177996,
      "grad_norm": 73.3003158569336,
      "learning_rate": 6.40114751226349e-09,
      "loss": 1.8214,
      "step": 6600
    },
    {
      "epoch": 2.3826208829712683,
      "grad_norm": 82.8016357421875,
      "learning_rate": 6.322310791871058e-09,
      "loss": 1.7912,
      "step": 6800
    },
    {
      "epoch": 2.452697967764541,
      "grad_norm": 68.94673156738281,
      "learning_rate": 6.243474071478626e-09,
      "loss": 1.7828,
      "step": 7000
    },
    {
      "epoch": 2.5227750525578134,
      "grad_norm": 80.68631744384766,
      "learning_rate": 6.164637351086194e-09,
      "loss": 1.7485,
      "step": 7200
    },
    {
      "epoch": 2.592852137351086,
      "grad_norm": 75.22706604003906,
      "learning_rate": 6.086194814295725e-09,
      "loss": 1.7786,
      "step": 7400
    },
    {
      "epoch": 2.662929222144359,
      "grad_norm": 71.51626586914062,
      "learning_rate": 6.007358093903293e-09,
      "loss": 1.7706,
      "step": 7600
    },
    {
      "epoch": 2.7330063069376314,
      "grad_norm": 134.83218383789062,
      "learning_rate": 5.928521373510862e-09,
      "loss": 1.7395,
      "step": 7800
    },
    {
      "epoch": 2.803083391730904,
      "grad_norm": 92.52971649169922,
      "learning_rate": 5.84968465311843e-09,
      "loss": 1.7468,
      "step": 8000
    },
    {
      "epoch": 2.8731604765241765,
      "grad_norm": 83.9453353881836,
      "learning_rate": 5.770847932725998e-09,
      "loss": 1.7468,
      "step": 8200
    },
    {
      "epoch": 2.9432375613174493,
      "grad_norm": 85.50732421875,
      "learning_rate": 5.6920112123335666e-09,
      "loss": 1.7269,
      "step": 8400
    },
    {
      "epoch": 3.0133146461107216,
      "grad_norm": 90.91653442382812,
      "learning_rate": 5.6131744919411355e-09,
      "loss": 1.7073,
      "step": 8600
    },
    {
      "epoch": 3.0833917309039944,
      "grad_norm": 90.9706802368164,
      "learning_rate": 5.534337771548703e-09,
      "loss": 1.7054,
      "step": 8800
    },
    {
      "epoch": 3.1534688156972672,
      "grad_norm": 81.1943588256836,
      "learning_rate": 5.455501051156271e-09,
      "loss": 1.6921,
      "step": 9000
    },
    {
      "epoch": 3.2235459004905396,
      "grad_norm": 77.4426498413086,
      "learning_rate": 5.37666433076384e-09,
      "loss": 1.6285,
      "step": 9200
    },
    {
      "epoch": 3.2936229852838124,
      "grad_norm": 90.48431396484375,
      "learning_rate": 5.29822179397337e-09,
      "loss": 1.6771,
      "step": 9400
    },
    {
      "epoch": 3.3637000700770847,
      "grad_norm": 75.54402923583984,
      "learning_rate": 5.219385073580939e-09,
      "loss": 1.6787,
      "step": 9600
    },
    {
      "epoch": 3.4337771548703575,
      "grad_norm": 244.44754028320312,
      "learning_rate": 5.140548353188507e-09,
      "loss": 1.667,
      "step": 9800
    },
    {
      "epoch": 3.50385423966363,
      "grad_norm": 100.16873168945312,
      "learning_rate": 5.061711632796076e-09,
      "loss": 1.6308,
      "step": 10000
    },
    {
      "epoch": 3.5739313244569026,
      "grad_norm": 72.78389739990234,
      "learning_rate": 4.982874912403643e-09,
      "loss": 1.6386,
      "step": 10200
    },
    {
      "epoch": 3.6440084092501754,
      "grad_norm": 202.41416931152344,
      "learning_rate": 4.904038192011212e-09,
      "loss": 1.6031,
      "step": 10400
    },
    {
      "epoch": 3.714085494043448,
      "grad_norm": 82.48982238769531,
      "learning_rate": 4.8252014716187804e-09,
      "loss": 1.6311,
      "step": 10600
    },
    {
      "epoch": 3.78416257883672,
      "grad_norm": 84.62272644042969,
      "learning_rate": 4.746364751226349e-09,
      "loss": 1.5839,
      "step": 10800
    },
    {
      "epoch": 3.854239663629993,
      "grad_norm": 71.36602783203125,
      "learning_rate": 4.667528030833917e-09,
      "loss": 1.5805,
      "step": 11000
    },
    {
      "epoch": 3.9243167484232657,
      "grad_norm": 201.88265991210938,
      "learning_rate": 4.588691310441486e-09,
      "loss": 1.5784,
      "step": 11200
    },
    {
      "epoch": 3.994393833216538,
      "grad_norm": 80.20748901367188,
      "learning_rate": 4.509854590049054e-09,
      "loss": 1.5724,
      "step": 11400
    },
    {
      "epoch": 4.06447091800981,
      "grad_norm": 86.8395767211914,
      "learning_rate": 4.431017869656622e-09,
      "loss": 1.5862,
      "step": 11600
    },
    {
      "epoch": 4.134548002803084,
      "grad_norm": 87.75743103027344,
      "learning_rate": 4.35218114926419e-09,
      "loss": 1.5711,
      "step": 11800
    },
    {
      "epoch": 4.204625087596356,
      "grad_norm": 82.63009643554688,
      "learning_rate": 4.273738612473721e-09,
      "loss": 1.5528,
      "step": 12000
    },
    {
      "epoch": 4.274702172389628,
      "grad_norm": 97.80199432373047,
      "learning_rate": 4.194901892081289e-09,
      "loss": 1.5271,
      "step": 12200
    },
    {
      "epoch": 4.344779257182902,
      "grad_norm": 94.12675476074219,
      "learning_rate": 4.116065171688858e-09,
      "loss": 1.5118,
      "step": 12400
    },
    {
      "epoch": 4.414856341976174,
      "grad_norm": 68.2523193359375,
      "learning_rate": 4.037228451296426e-09,
      "loss": 1.531,
      "step": 12600
    },
    {
      "epoch": 4.484933426769446,
      "grad_norm": 83.85641479492188,
      "learning_rate": 3.958391730903994e-09,
      "loss": 1.4986,
      "step": 12800
    },
    {
      "epoch": 4.555010511562719,
      "grad_norm": 78.17858123779297,
      "learning_rate": 3.8795550105115625e-09,
      "loss": 1.4918,
      "step": 13000
    },
    {
      "epoch": 4.625087596355992,
      "grad_norm": 115.51760864257812,
      "learning_rate": 3.800718290119131e-09,
      "loss": 1.494,
      "step": 13200
    },
    {
      "epoch": 4.695164681149264,
      "grad_norm": 79.49065399169922,
      "learning_rate": 3.7218815697266993e-09,
      "loss": 1.5043,
      "step": 13400
    },
    {
      "epoch": 4.765241765942537,
      "grad_norm": 101.60753631591797,
      "learning_rate": 3.6430448493342674e-09,
      "loss": 1.4944,
      "step": 13600
    },
    {
      "epoch": 4.83531885073581,
      "grad_norm": 67.72956848144531,
      "learning_rate": 3.564208128941836e-09,
      "loss": 1.4873,
      "step": 13800
    },
    {
      "epoch": 4.905395935529082,
      "grad_norm": 87.92948150634766,
      "learning_rate": 3.485371408549404e-09,
      "loss": 1.4569,
      "step": 14000
    },
    {
      "epoch": 4.9754730203223545,
      "grad_norm": 64.7652359008789,
      "learning_rate": 3.4065346881569728e-09,
      "loss": 1.4618,
      "step": 14200
    },
    {
      "epoch": 5.045550105115627,
      "grad_norm": 72.74520111083984,
      "learning_rate": 3.327697967764541e-09,
      "loss": 1.4493,
      "step": 14400
    },
    {
      "epoch": 5.1156271899089,
      "grad_norm": 69.39006042480469,
      "learning_rate": 3.248861247372109e-09,
      "loss": 1.4443,
      "step": 14600
    },
    {
      "epoch": 5.185704274702172,
      "grad_norm": 71.9982681274414,
      "learning_rate": 3.1700245269796777e-09,
      "loss": 1.4321,
      "step": 14800
    },
    {
      "epoch": 5.255781359495445,
      "grad_norm": 78.89315795898438,
      "learning_rate": 3.0911878065872454e-09,
      "loss": 1.4352,
      "step": 15000
    },
    {
      "epoch": 5.325858444288718,
      "grad_norm": 81.22355651855469,
      "learning_rate": 3.0123510861948144e-09,
      "loss": 1.4098,
      "step": 15200
    },
    {
      "epoch": 5.39593552908199,
      "grad_norm": 87.02701568603516,
      "learning_rate": 2.933908549404345e-09,
      "loss": 1.45,
      "step": 15400
    },
    {
      "epoch": 5.466012613875263,
      "grad_norm": 99.46730041503906,
      "learning_rate": 2.8550718290119127e-09,
      "loss": 1.4645,
      "step": 15600
    },
    {
      "epoch": 5.536089698668535,
      "grad_norm": 112.48688507080078,
      "learning_rate": 2.7762351086194813e-09,
      "loss": 1.4174,
      "step": 15800
    },
    {
      "epoch": 5.606166783461808,
      "grad_norm": 63.51983642578125,
      "learning_rate": 2.6973983882270495e-09,
      "loss": 1.4058,
      "step": 16000
    },
    {
      "epoch": 5.676243868255081,
      "grad_norm": 69.47407531738281,
      "learning_rate": 2.6185616678346177e-09,
      "loss": 1.427,
      "step": 16200
    },
    {
      "epoch": 5.746320953048353,
      "grad_norm": 68.47804260253906,
      "learning_rate": 2.5397249474421862e-09,
      "loss": 1.4334,
      "step": 16400
    },
    {
      "epoch": 5.816398037841626,
      "grad_norm": 90.2673110961914,
      "learning_rate": 2.4608882270497544e-09,
      "loss": 1.4122,
      "step": 16600
    },
    {
      "epoch": 5.886475122634899,
      "grad_norm": 60.9555778503418,
      "learning_rate": 2.382051506657323e-09,
      "loss": 1.4135,
      "step": 16800
    },
    {
      "epoch": 5.956552207428171,
      "grad_norm": 113.80003356933594,
      "learning_rate": 2.303214786264891e-09,
      "loss": 1.3731,
      "step": 17000
    },
    {
      "epoch": 6.026629292221443,
      "grad_norm": 75.94062805175781,
      "learning_rate": 2.2243780658724593e-09,
      "loss": 1.3976,
      "step": 17200
    },
    {
      "epoch": 6.0967063770147165,
      "grad_norm": 69.50025177001953,
      "learning_rate": 2.145541345480028e-09,
      "loss": 1.4161,
      "step": 17400
    },
    {
      "epoch": 6.166783461807989,
      "grad_norm": 58.354644775390625,
      "learning_rate": 2.066704625087596e-09,
      "loss": 1.3891,
      "step": 17600
    },
    {
      "epoch": 6.236860546601261,
      "grad_norm": 70.5787353515625,
      "learning_rate": 1.9878679046951647e-09,
      "loss": 1.3914,
      "step": 17800
    },
    {
      "epoch": 6.3069376313945344,
      "grad_norm": 61.01400375366211,
      "learning_rate": 1.909425367904695e-09,
      "loss": 1.3957,
      "step": 18000
    },
    {
      "epoch": 6.377014716187807,
      "grad_norm": 63.336448669433594,
      "learning_rate": 1.8305886475122636e-09,
      "loss": 1.3954,
      "step": 18200
    },
    {
      "epoch": 6.447091800981079,
      "grad_norm": 64.59988403320312,
      "learning_rate": 1.7517519271198315e-09,
      "loss": 1.3964,
      "step": 18400
    },
    {
      "epoch": 6.5171688857743515,
      "grad_norm": 63.213783264160156,
      "learning_rate": 1.6729152067274e-09,
      "loss": 1.3848,
      "step": 18600
    },
    {
      "epoch": 6.587245970567625,
      "grad_norm": 64.03404235839844,
      "learning_rate": 1.5940784863349683e-09,
      "loss": 1.4039,
      "step": 18800
    },
    {
      "epoch": 6.657323055360897,
      "grad_norm": 71.82128143310547,
      "learning_rate": 1.5152417659425367e-09,
      "loss": 1.3946,
      "step": 19000
    },
    {
      "epoch": 6.727400140154169,
      "grad_norm": 60.97812271118164,
      "learning_rate": 1.436405045550105e-09,
      "loss": 1.3829,
      "step": 19200
    },
    {
      "epoch": 6.797477224947443,
      "grad_norm": 55.707035064697266,
      "learning_rate": 1.3575683251576734e-09,
      "loss": 1.364,
      "step": 19400
    },
    {
      "epoch": 6.867554309740715,
      "grad_norm": 110.37628173828125,
      "learning_rate": 1.2787316047652418e-09,
      "loss": 1.3633,
      "step": 19600
    },
    {
      "epoch": 6.937631394533987,
      "grad_norm": 71.63675689697266,
      "learning_rate": 1.19989488437281e-09,
      "loss": 1.3635,
      "step": 19800
    },
    {
      "epoch": 7.00770847932726,
      "grad_norm": 61.477088928222656,
      "learning_rate": 1.1210581639803783e-09,
      "loss": 1.3835,
      "step": 20000
    },
    {
      "epoch": 7.077785564120533,
      "grad_norm": 64.05018615722656,
      "learning_rate": 1.0422214435879467e-09,
      "loss": 1.371,
      "step": 20200
    },
    {
      "epoch": 7.147862648913805,
      "grad_norm": 59.59954833984375,
      "learning_rate": 9.633847231955149e-10,
      "loss": 1.3786,
      "step": 20400
    },
    {
      "epoch": 7.217939733707078,
      "grad_norm": 78.21539306640625,
      "learning_rate": 8.849421864050455e-10,
      "loss": 1.3682,
      "step": 20600
    },
    {
      "epoch": 7.28801681850035,
      "grad_norm": 146.09030151367188,
      "learning_rate": 8.061054660126139e-10,
      "loss": 1.3707,
      "step": 20800
    },
    {
      "epoch": 7.358093903293623,
      "grad_norm": 60.67129135131836,
      "learning_rate": 7.272687456201821e-10,
      "loss": 1.3684,
      "step": 21000
    },
    {
      "epoch": 7.428170988086896,
      "grad_norm": 55.07486343383789,
      "learning_rate": 6.484320252277505e-10,
      "loss": 1.3621,
      "step": 21200
    },
    {
      "epoch": 7.498248072880168,
      "grad_norm": 60.74055099487305,
      "learning_rate": 5.695953048353188e-10,
      "loss": 1.3793,
      "step": 21400
    },
    {
      "epoch": 7.568325157673441,
      "grad_norm": 61.56096267700195,
      "learning_rate": 4.907585844428871e-10,
      "loss": 1.3787,
      "step": 21600
    },
    {
      "epoch": 7.6384022424667135,
      "grad_norm": 61.33281707763672,
      "learning_rate": 4.1192186405045543e-10,
      "loss": 1.3573,
      "step": 21800
    },
    {
      "epoch": 7.708479327259986,
      "grad_norm": 58.40278625488281,
      "learning_rate": 3.330851436580238e-10,
      "loss": 1.4181,
      "step": 22000
    },
    {
      "epoch": 7.778556412053258,
      "grad_norm": 71.1231918334961,
      "learning_rate": 2.5424842326559214e-10,
      "loss": 1.3653,
      "step": 22200
    },
    {
      "epoch": 7.848633496846531,
      "grad_norm": 69.02555847167969,
      "learning_rate": 1.7541170287316049e-10,
      "loss": 1.3745,
      "step": 22400
    },
    {
      "epoch": 7.918710581639804,
      "grad_norm": 59.334861755371094,
      "learning_rate": 9.696916608269096e-11,
      "loss": 1.4086,
      "step": 22600
    },
    {
      "epoch": 7.988787666433076,
      "grad_norm": 55.608524322509766,
      "learning_rate": 1.8132445690259285e-11,
      "loss": 1.3687,
      "step": 22800
    }
  ],
  "logging_steps": 200,
  "max_steps": 22832,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.49659012939776e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
